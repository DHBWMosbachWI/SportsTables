{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Web-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-17T15:15:42.885802Z",
     "end_time": "2023-05-17T15:22:51.067677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1982\n",
      "Saved Premier-League-Stats_matchlogs_for_1982-1983.csv\n",
      "Saved Premier-League-Stats_results1982-198391_overall_1982-1983.csv\n",
      "Saved Premier-League-Stats_results1982-198391_home_away_1982-1983.csv\n",
      "not possible\n",
      "1983\n",
      "not possible\n",
      "1984\n",
      "not possible\n",
      "1985\n",
      "not possible\n",
      "1986\n"
     ]
    }
   ],
   "source": [
    "### extract all soccer data\n",
    "\n",
    "# code für die player stats - scraped nur eine tabelle, wie im alten code\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "from web_scraping import get_soccer_leagues_ids, get_soccer_league_season_standings, get_soccer_teams_url, get_player_season_stats2\n",
    "\n",
    "current_season = 2022\n",
    "soccer_league_ids = get_soccer_leagues_ids()\n",
    "\n",
    "for year in range(1982,2000):\n",
    "    print(year)\n",
    "    try:\n",
    "        for soccer_league_id in soccer_league_ids:\n",
    "            if soccer_league_id[\"league_name\"] == \"Fußball-Bundesliga\" or soccer_league_id[\"league_name\"] == \"Big 5 European Leagues Combined\":\n",
    "                continue\n",
    "\n",
    "            # get player stats\n",
    "            df = get_player_season_stats2(year, current_season, soccer_league_id[\"league_id\"], soccer_league_id[\"league_stats_name\"])\n",
    "            df.to_csv(join(os.environ[\"SportsTables\"], \"soccer\", f\"{soccer_league_id['league_name'].replace(' ','-')}_player_stats_{year}.csv\"), index=False)\n",
    "\n",
    "            time.sleep(3)\n",
    "    except:\n",
    "        print(\"not possible\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Premier-League-Stats_results1982-198391_overall.csv\n",
      "Saved Premier-League-Stats_results1982-198391_home_away.csv\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB7C6EB0>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB80B070>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB80B550>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB7C6FD0>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB7C66A0>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB80B4C0>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n",
      "not possible\n",
      "HTTPConnectionPool(host='localhost', port=50449): Max retries exceeded with url: /session/ab91ca3ea5833bb3e544ebc5f907786c/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000273AB80BAF0>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))\n"
     ]
    }
   ],
   "source": [
    "# zeigt den Fehler, wenn alle Tabellen von den player stats geladen werden sollen\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "from web_scraping import get_player_season_stats, get_soccer_leagues_ids, get_soccer_league_season_standings\n",
    "\n",
    "\n",
    "current_season = 2022\n",
    "soccer_league_ids = get_soccer_leagues_ids()\n",
    "\n",
    "for year in range(1982, 2000):\n",
    "    for soccer_league_id in soccer_league_ids:\n",
    "        try:\n",
    "            standings = get_soccer_league_season_standings(year, current_season, soccer_league_id[\"league_id\"], soccer_league_id[\"league_stats_name\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            if hasattr(e, 'message'):\n",
    "                print(e.message)\n",
    "            else:\n",
    "                print ('not possible')\n",
    "                print(e)\n",
    "                continue\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T13:05:43.582022Z",
     "end_time": "2023-05-17T13:07:50.019135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n",
      "Saved Premier-League-Stats_stats_standard_9_1999-2000.csv\n",
      "Saved Premier-League-Stats_matchlogs_for_1999-2000.csv\n",
      "Saved Premier-League-Stats_stats_keeper_9_1999-2000.csv\n",
      "Saved Premier-League-Stats_stats_shooting_9_1999-2000.csv\n",
      "Saved Premier-League-Stats_stats_playing_time_9_1999-2000.csv\n",
      "Saved Premier-League-Stats_stats_misc_9_1999-2000.csv\n",
      "Saved Premier-League-Stats_results1999-200091_overall_1999-2000.csv\n",
      "Saved Premier-League-Stats_results1999-200091_home_away_1999-2000.csv\n",
      "not possible\n"
     ]
    }
   ],
   "source": [
    "# zeigt wie die player stats gescraped werden (nur premier league), also alle tabellen und dann alle Tabellen mit dem gleichen Namen werden zusammengefügt\n",
    "# 1 Jahr geht, aber mehr dann nicht\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "from web_scraping import get_soccer_leagues_ids, get_soccer_league_season_standings, get_soccer_teams_url, get_player_season_stats\n",
    "\n",
    "current_season = 2022\n",
    "soccer_league_ids = get_soccer_leagues_ids()\n",
    "\n",
    "for year in range(1999,2000):\n",
    "    print(year)\n",
    "    try:\n",
    "        for soccer_league_id in soccer_league_ids:\n",
    "            if soccer_league_id[\"league_name\"] == \"Fußball-Bundesliga\" or soccer_league_id[\"league_name\"] == \"Big 5 European Leagues Combined\":\n",
    "                continue\n",
    "\n",
    "            # get player stats\n",
    "            df = get_player_season_stats(year, current_season, soccer_league_id[\"league_id\"], soccer_league_id[\"league_stats_name\"])\n",
    "            df.to_csv(join(os.environ[\"SportsTables\"], \"soccer\", f\"{soccer_league_id['league_name'].replace(' ','-')}_player_stats_{year}.csv\"), index=False)\n",
    "\n",
    "            time.sleep(3)\n",
    "    except:\n",
    "        print(\"not possible\")\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-17T15:10:17.881172Z",
     "end_time": "2023-05-17T15:13:31.578515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# scraped all table data without getting httpconnectionpool error\n",
    "# 464 tables\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "leagues = {\n",
    "    'premier_league': list(range(1982, 2000)),\n",
    "    'ligue_1': list(range(1989, 2000)),\n",
    "    'serie_a': list(range(1994, 2000)),\n",
    "    'la_liga': list(range(1993, 2000)),\n",
    "    'fussball-bundesliga': list(range(1988, 2000)),\n",
    "    'champions-league': list(range(1992, 2000)),\n",
    "\n",
    "}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "for league, years in leagues.items():\n",
    "    for year in years:\n",
    "        if league == 'premier_league':\n",
    "            url = f'https://fbref.com/en/comps/9/{year}-{year+1}/{year}-{year+1}-Premier-League-Stats'\n",
    "        elif league == 'ligue_1':\n",
    "            url = f'https://fbref.com/en/comps/13/{year}-{year+1}/{year}-{year+1}-Ligue-1-Stats'\n",
    "        elif league == 'serie_a':\n",
    "            url = f'https://fbref.com/en/comps/11/{year}-{year+1}/{year}-{year+1}-Serie-A-Stats'\n",
    "        elif league == 'la_liga':\n",
    "            url = f'https://fbref.com/en/comps/12/{year}-{year+1}/{year}-{year+1}-La-Liga-Stats'\n",
    "        elif league == 'fussball-bundesliga':\n",
    "            url = f'https://fbref.com/en/comps/20/{year}-{year+1}/{year}-{year+1}-Bundesliga-Stats'\n",
    "        elif league == 'champions-league':\n",
    "            url = f'https://fbref.com/en/comps/8/{year}-{year+1}/{year}-{year+1}-Champions-League-Stats'\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                table_id = table.get('id')\n",
    "                if table_id is not None:\n",
    "                    df = pd.read_html(str(table))[0]\n",
    "                    name = f'{league}_{table_id}_{year}.csv'\n",
    "                    df.to_csv(name, index=False)\n",
    "                    print(f'Saved {name}')\n",
    "\n",
    "        except:\n",
    "            print(\"not possible\")\n",
    "            pass\n",
    "\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-12T16:31:33.547732Z",
     "end_time": "2023-05-12T16:42:29.393766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# scraped extra tables without getting http error\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "leagues = {\n",
    "    'premier_league': list(range(1982, 2000)),\n",
    "    'ligue_1': list(range(1989, 2000)),\n",
    "    'serie_a': list(range(1994, 2000)),\n",
    "    'la_liga': list(range(1993, 2000)),\n",
    "    'fussball-bundesliga': list(range(1988, 2000)),\n",
    "    'champions-league': list(range(1992, 2000)),\n",
    "\n",
    "}\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "for league, years in leagues.items():\n",
    "    for year in years:\n",
    "        if league == 'premier_league':\n",
    "            url = f'https://fbref.com/en/comps/9/{year}-{year+1}/schedule/{year}-{year+1}-Premier--Scores-and-Fixtures'\n",
    "        elif league == 'ligue_1':\n",
    "            url = f'https://fbref.com/en/comps/13/{year}-{year+1}/schedule/{year}-{year+1}-Ligue-1-Scores-and-Fixtures'\n",
    "        elif league == 'serie_a':\n",
    "            url = f'https://fbref.com/en/comps/11/{year}-{year+1}/schedule/{year}-{year+1}-Serie-A-Scores-and-Fixtures'\n",
    "        elif league == 'la_liga':\n",
    "            url = f'https://fbref.com/en/comps/12/{year}-{year+1}/schedule/{year}-{year+1}-La-Liga-Scores-and-Fixtures'\n",
    "        elif league == 'fussball-bundesliga':\n",
    "            url = f'https://fbref.com/en/comps/20/{year}-{year+1}/schedule/{year}-{year+1}-Bundesliga-Scores-and-Fixtures'\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                table_id = table.get('id')\n",
    "                if table_id is not None:\n",
    "                    df = pd.read_html(str(table))[0]\n",
    "                    name = f'{league}_{table_id}_{year}.csv'\n",
    "                    df.to_csv(name, index=False)\n",
    "                    print(f'Saved {name}')\n",
    "\n",
    "        except:\n",
    "            print(\"not possible\")\n",
    "            pass\n",
    "\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract column headers from different tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T16:31:24.627542Z",
     "end_time": "2023-05-05T16:31:24.745434Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "tables = [\"player_stats\", \"season_standings\"]\n",
    "\n",
    "for table in tables:\n",
    "    column_names = []\n",
    "    for found_table in glob(join(os.environ[\"SportsTables\"], \"soccer\", f\"*{table}_*.csv\")):\n",
    "        column_names.extend(list(pd.read_csv(found_table).columns))\n",
    "    \n",
    "    column_names = list(set(column_names))\n",
    "    metadata[table] = {}\n",
    "    for column_name in column_names:\n",
    "        metadata[table][column_name] = None\n",
    "    \n",
    "if os.path.isfile(\"metadata.json\"):\n",
    "    print(\"metadata.json exists already!!! \")\n",
    "else:\n",
    "    with open(\"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract valid header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-04T22:29:45.919652Z",
     "end_time": "2023-05-04T22:29:45.934716Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "import os \n",
    "#os.environ[\"SPORTS_DB\"] = \"/ext/daten-wi/slangenecker/SportsTables\"\n",
    "os.environ[\"SPORTS_DB\"] = \"C:/Users/jwuel/DataspellProjects/SportsTables\"\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "def get_sportsDB_soccer_type_mappings():\n",
    "    with open(join(os.environ[\"SPORTS_DB\"], \"Soccer\", \"Soccer-Relations.json\"), \"r\") as f:\n",
    "        type_mappings = json.load(f)[\"soccer-big5Leagues-Players-2021-2022.csv\"][\"columns\"][0]\n",
    "        return type_mappings\n",
    "\n",
    "\n",
    "def get_all_sportsDB_soccer_tables(only_file_names:bool=False):\n",
    "    result = glob(join(os.environ[\"SPORTS_DB\"], \"Soccer\", \"soccerPlayerScraping\", \"*.csv\"))\n",
    "    if only_file_names:\n",
    "        return [table.split(\"/\")[-1] for table in result]\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def load_sportsDB_soccer_table(table_name:str, only_headers:bool=False):\n",
    "    df = pd.read_csv(join(os.environ[\"SPORTS_DB\"], \"Soccer\", \"soccerPlayerScraping\", table_name))\n",
    "    if only_headers:\n",
    "        return df.columns\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_to_sem_type = get_sportsDB_soccer_type_mappings()\n",
    "\n",
    "results = {}\n",
    "for table in get_all_sportsDB_soccer_tables(True):\n",
    "    print(table)\n",
    "    results[table.split(\".csv\")[0]] = {}\n",
    "    col_names = load_sportsDB_soccer_table(table, True)\n",
    "    for col_num, col_name in enumerate(col_names):\n",
    "        try:\n",
    "            results[table.split(\".csv\")[0]][f\"column_{col_num}\"] = { \"semanticType\": col_name_to_sem_type[col_name]}\n",
    "        except:\n",
    "            print(col_name)\n",
    "    \n",
    "with open(join(os.environ[\"SPORTS_DB\"], \"Soccer\", \"sportsDB_type_sportsDB.json\"), \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
